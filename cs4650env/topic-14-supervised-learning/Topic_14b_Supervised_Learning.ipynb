{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090cf758",
   "metadata": {},
   "source": [
    "# Supervised Learning: Classifiers\n",
    "\n",
    "In the last lecture, we talked about classifiers.  In this lecture, we will actually _use_ the classifiers!\n",
    "\n",
    "Recall that classifiers accept a sample value, then determine if the sample belongs to _class 1_ or _class 2_.  It makes this decision based on a training set which included not only samples, but for each sample an indication of which class that sample belonged to.\n",
    "\n",
    "The classifier may not (i.e. probably will not) be perfect, it may have some errors.  The correct responses include:\n",
    "\n",
    "* True Positives: samples that the classifier said should be in _class 1_, and the sample indeed belongs in _class 1_.\n",
    "\n",
    "* True Negatives: samples that the classifier said should be in _class 2_, and the sample indeed belongs in _class 2_.\n",
    "\n",
    "The incorrect responses are:\n",
    "\n",
    "* False Positives: samples that the classifier said should be in _class 1_, but actually the sample belongs in _class 2_.\n",
    "\n",
    "* False Negatives: samples that the classifier said should be in _class 2_, but actually the sample belongs in _class 1_.\n",
    "\n",
    "Naturally, we want the classifier to always compute the correct response, so that the False Positives and False Negatives never occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd6970",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "For this chapter, the book uses data from the Lending Club, which is a peer-to-peer lending company.  Some members of the company are looking for loans, and other members of the company are looking to loan money.  A client (potentially a borrower) submits an application for a loan.  The company assesses the risk, and may decide to accept or reject the application.  If it is accepted, then an investor (potentially a lender) may decide to loan the full amount, or perhaps a partial amount.  There may be multiple investors funding the loan.\n",
    "\n",
    "The question we want to answer is, \"Given the data on the application, will this loan be fully funded?\"  This is a classification problem, the answer will be either 'Yes' or 'No'.  Since the answer has only two possible values, this is a _binary_ classifier.  If there were more that two possible answers, this would be a _multiclass_ classifier.\n",
    "\n",
    "In a few moments, we will start poking into the data, but first we will set up the Notebook.  For now, we will load the following packages.  Later we might add some additional packages.\n",
    "\n",
    "* pickle -- this is used to read and write Python objects to binary data files.  This is similar to JSON and Javascript, but the file format is not human-readable.\n",
    "\n",
    "* several tools from scikit-learn -- neighbors (to get the KNN classifier), datasets, metrics.\n",
    "\n",
    "* matplotlib -- to draw graphs\n",
    "\n",
    "* numpy -- numeric processing procedures and structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552e9269",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m neighbors\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m metrics\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a387f7",
   "metadata": {},
   "source": [
    "Things have changed a lot in this ecosystem since the book was written.  So some of the packages we use are written to use code that will at some point soon will be deprecated.  So we will see several warning messages.  We can simply ignore these messages for now, or we can modify the code to not use those features.  But the code which uses these features are some of these packages that we have imported.  We cannot upgrade the code, we are waiting for the authors of those packages to do this.\n",
    "\n",
    "So for now, we just want to turn off the message.  The following code will turn off all warning messages.  Not the best practice, but I've looked at the messages, found they don't apply right now, so then I came back here and turned off the warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22eac6e",
   "metadata": {},
   "source": [
    "This next cell loads the data from the pkl file.  The data consists of two arrays:\n",
    "\n",
    "* The first array, which we will call 'x', is a 2-dimensional array.  Each row in the array is one loan application, and each column is one of the attributes from the application.  We don't know what each of the columns represent, but for this application we don't need to know!\n",
    "\n",
    "* The second array, which we will call 'y', is a 1-dimensional array indicating whether the application was fully funded (1.0) or not fully funded (-1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e544a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ofname = open('dataset_small.pkl', 'rb')\n",
    "(x, y) = pickle.load(ofname, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c743a72",
   "metadata": {},
   "source": [
    "We can take a peek at the x array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7178e44d",
   "metadata": {},
   "source": [
    "... and the y array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd08ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbfc2a",
   "metadata": {},
   "source": [
    "These arrays are Numpy arrays.\n",
    "\n",
    "We don't know the sizes of the arrays, but this would be good to know.  We can ask for an array's _shape_, which gives us the number of entries in the array.\n",
    "\n",
    "Since _x_ is a 2-dimensional array, _shape_ will return an array, 2 long, giving the counts for the two dimensions.  In the following code, we separate and then print out these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983096b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = x.shape[1]\n",
    "N = x.shape[0]\n",
    "print(f'dims: {dims}, samples: {N}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3925b",
   "metadata": {},
   "source": [
    "## KNN Classification\n",
    "\n",
    "Let's start by making a K-Nearest Neighbor classifier.  We will arbitrarily use _k = 11_, so when we ask for a prediction, it will find the 11 nearest neighbors, then merge the results to come up with a final result.\n",
    "\n",
    "In the following, we create the classifier, then train it with the data we loaded above.  We then do a prediction _with the same data we used to train the classifier_.  We will then look at the last sample, just for example, to see what the actual and what the predicted results are.  Ideally, these would have the same value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of K-nearest neighbor classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "\n",
    "# Train the classifier\n",
    "knn.fit(x, y)\n",
    "\n",
    "# Compute the prediction according to the model\n",
    "yhat = knn.predict(x)\n",
    "\n",
    "# Check the results on the last sample\n",
    "print(f'Predicted value: {yhat[-1]}, real target: {y[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75250d3b",
   "metadata": {},
   "source": [
    "Great!  We predicted that the last application would not be funded, and it wasn't!  What about all of the other cases?\n",
    "\n",
    "Classifiers have a method, _score_, which is passed an array of applications (input samples) and a vector of expected values (output results).  The method will do a _predict_, then compute the percentage of predictions actually match the expected values.\n",
    "\n",
    "We will pass the original data, both the applications and results, to score.  So the same data we used to train the classifier is being used to test the classifier.  Ideally, our score should be 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe76961",
   "metadata": {},
   "source": [
    "Why is the score not 100%?\n",
    "\n",
    "We are looking at the 11 nearest neighbors, then computing some form of averaging of the result.  One of the nearest neighbors will in fact be the input sample.  However, we are 'blurring' the results with the other 10 values.  Perhaps enough of the other neighbors have the opposite value, so then the classifier will return the wrong result.  Evidently, in about 17% of the cases, this is what is happening!\n",
    "\n",
    "In a bit we will try tuning the classifier, and hopefully we will get a better score.\n",
    "\n",
    "But in the mean time, the book wanted to explore the data a little more.  The question they want to answer is, \"What percentage of applications are funded?\"  They built a pie chart showing the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(np.c_[np.sum(np.where(y==1,1,0)),np.sum(np.where(y==-1,1,0))][0],\n",
    "        labels=['Not fully funded','Full amount'],\n",
    "        colors=['g','r'],\n",
    "        shadow=False,\n",
    "        autopct ='%.2f' )\n",
    "plt.gcf().set_size_inches((6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d75329",
   "metadata": {},
   "source": [
    "Interestingly enough, about 82% of the applications were funded.  Our classifier had about 83% accuracy.\n",
    "\n",
    "If we built a classifier that just said that every application was fully funded, we would be right about 82% of the time, which is just about as good as our fancy classifier!  That's not very encouraging!  (And it fact, it is a little coincidental that the numbers were so similar.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983cf6e1",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "We might glean additional information, and perhaps some insights in improving the situation, by looking closer at what we were getting wrong.  The first step of doing this is to consider the _confusion matrix_.  Recall that this matrix displays four values:\n",
    "\n",
    "* How many times did the prediction say the answer was 'Yes' when the actual value was 'Yes'?  These cases are correct predictions, the _True Positives_.\n",
    "\n",
    "* How many times did the prediction say the answer was 'No' when the actual value was 'No'?  These are also correct predictions, the _True Negatives_.\n",
    "\n",
    "* How many times did the prediction say the answer was 'Yes', when the actual value was 'No'?  These are incorrect predictions, the _False Positives_.\n",
    "\n",
    "* How many times did the prediction say the answer was 'No', when the actual value was 'Yes'?  These are incorrect predictions, the _False Negatives_.\n",
    "\n",
    "These values are easy to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum(np.logical_and(yhat == -1, y == -1))\n",
    "TN = np.sum(np.logical_and(yhat == 1, y == 1))\n",
    "FP = np.sum(np.logical_and(yhat == -1, y == 1))\n",
    "FN = np.sum(np.logical_and(yhat == 1, y == -1))\n",
    "print(f'TP: {TP:4}, FP: {FP:4}')\n",
    "print(f'FN: {FN:4}, TN: {TN:4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf5c5c",
   "metadata": {},
   "source": [
    "While this code was pretty simple, the confusion matrix is a common metric, so they have actually built a function which computes this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(yhat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a35e06",
   "metadata": {},
   "source": [
    "Recall that right now, we are running the prediction on exactly the same data as we used to train the model, so why is the score so low?  Looking at the matrix, the major problem is the false positives: in too many cases, the prediction thinks the answer should be 'Yes', when really it should be 'No'.\n",
    "\n",
    "Recall how the _KNN_ classifier works:\n",
    "\n",
    "* It keeps the list of training samples, along with their output values.\n",
    "\n",
    "* When a sample is being predicted, it finds a number of these training vectors, the _k_ closest ones.  It then performs some type of averaging (different implementations use different averaging techniques), to pick an answer.\n",
    "\n",
    "In too many cases, when we are gathering the list of nearest neighbors, we gather too many that belong to the other set, the wrong answer.\n",
    "\n",
    "Remember also how skewed the data set is: for every 'No' answer, there are four 'Yes' answers.  So when we find neighbors around a 'No' sample, there is a good chance that the majority of these neighbors will belong to the 'Yes' state.\n",
    "\n",
    "So let's vary the number of neighbors.  Perhaps we will get better results with fewer neighbors confusing the issue!\n",
    "\n",
    "I took the code from the book, and wrote it as a function, so that it is easier to run a number of cases.  And in fact, I split it into two functions, one to perform the calculation and the other to print the results.  The reason for that is because some of our examples later on will use the same reporting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e017e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary the number of neighbors in the classifier\n",
    "\n",
    "# This prints the statistics, comparing the result (output of the predictor) with\n",
    "# the answer (the actual values for those samples)\n",
    "def classifierStats(label,result,answer):\n",
    "    print(f'--- {label} Stats ---')\n",
    "    print(f'classification accuracy: {100*metrics.accuracy_score(result, answer):.2f}%')\n",
    "    print(f'confusion matrix:\\n{metrics.confusion_matrix(result, answer)}')\n",
    "\n",
    "# Given a dataset (x) and the actual outputs (y), build a KNN with a given number\n",
    "# of nearest neighbors (num_neigh), then print the statistics\n",
    "def classifyWithNeighbors(x, y, num_neigh):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = num_neigh)\n",
    "    knn.fit(x, y)\n",
    "    yhat = knn.predict(x)\n",
    "    classifierStats(f'{num_neigh} Nearest Neighbors', yhat, y)\n",
    "    \n",
    "classifyWithNeighbors(x, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df9d12",
   "metadata": {},
   "source": [
    "Wow!  This is great results!\n",
    "\n",
    "Not really, unfortunately.  The samples we are trying to predict are exactly the same samples we used to train.  And since we are only asking for the one nearest neighbor, we are going to find in each case the same sample in the training set, so of course the answer is correct.\n",
    "\n",
    "However, we will see later how a KNN with _K = 1_ is still a valid option for a classifier.\n",
    "\n",
    "We can make a loop to try several different sizes, to see if we can find a pattern.  One thing they said about KNN, it is best to have an odd number of neighbors (to break ties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11, 2):\n",
    "    classifyWithNeighbors(x, y, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c6bb7e",
   "metadata": {},
   "source": [
    "Here we see that smaller neighborhood sizes work better.  For other problems, with other data, we may find different results.  Sometimes larger numbers of neighbors works better!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a42b",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "As we stated above, a big flaw with the approach we have used so far is that we are doing a prediction on exactly the same data we used for training!  So we don't really know how well our classifier is working.  We want the classifier to work well with _new_ data, data we haven't yet seen.\n",
    "\n",
    "Actually, we can solve this problem by splitting the dataset into two parts: the training part and the testing part.  The training part will be used to train (fit) the classifier.  We can then run the samples of the testing part through the predictor, then compare the results with the known answers for each of those testing samples.  This will tell us how well the classifier works with data _not_ used in the training.\n",
    "\n",
    "We will start by permuting the samples.  Why?  Perhaps the input data is sorted in some manner.  For example, maybe it is sorted by date, and maybe the results vary based on time of year.  If so, then if we just split the dataset, we might be training with 'early in the year' data, but then testing with 'late in the year' data.  So the training would be skewed to correctly predict early in the year samples.\n",
    "\n",
    "Numpy has a method for computing a permutation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a permutation list\n",
    "perm = np.random.permutation(10)\n",
    "perm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362172fe",
   "metadata": {},
   "source": [
    "The following makes the actual permutation list we will use, the size matches the size of our data.\n",
    "\n",
    "The next thing we need is the _split point_: at what index do we divide between the training set and the testing set.  We want a given percentage of data in the training set, so we compute the index that is that percentage of the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the data, just in case the data was coming in some sort of order\n",
    "\n",
    "perm = np.random.permutation(y.size)\n",
    "\n",
    "# PRC gives the percentage of values that we want in the training set\n",
    "PRC = 0.7\n",
    "split_point = int(np.ceil(y.shape[0] * PRC))\n",
    "split_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322de7d9",
   "metadata": {},
   "source": [
    "To build the training set, we want to use all of the entries in the permutation list, from 0 up to the split point, to index into the dataset.\n",
    "\n",
    "To build the test set, we want the portion of the permutation list, from the split point to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26a88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training and test sets\n",
    "X_train = x[perm[:split_point], : ]\n",
    "Y_train = y[perm[:split_point]]\n",
    "X_test = x[perm[split_point:], : ]\n",
    "Y_test = y[perm[split_point:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40acc66",
   "metadata": {},
   "source": [
    "Let's now look at the shape of the data, to verify that our splitting worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X training shape: {X_train.shape}, Y training shape: {Y_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}, Y test shape: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67102c3e",
   "metadata": {},
   "source": [
    "We can now train the classifier with the training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbf024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X_train, Y_train)\n",
    "yhat = knn.predict(X_train)\n",
    "classifierStats('Training', yhat, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7a165",
   "metadata": {},
   "source": [
    "...then evaluate the prediction using the testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db766b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the test case\n",
    "yhat = knn.predict(X_test)\n",
    "classifierStats('Testing', yhat, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33f0c6",
   "metadata": {},
   "source": [
    "Well, that's not so great!  With _K = 1_, the KNN classifier didn't do so well with unseen samples (the testing samples vs the training samples).  Let's explore varying the neighborhood size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a4cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RUNS = 20\n",
    "scores = []\n",
    "xvals = list(range(1, 2 * NUM_RUNS, 2));\n",
    "for size in xvals:\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = size)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    yhat = knn.predict(X_test)\n",
    "    scores.append(100 * metrics.accuracy_score(yhat, Y_test))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "plt.plot(xvals, scores);\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Neighborhood size')\n",
    "plt.xticks(xvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154ae4a",
   "metadata": {},
   "source": [
    "As we can see, for this data, KNN did a really poor job for _K = 1_, while when just using the testing data, that wasn't such a good idea.  But we get good results for size 5 and above (again, we prefer an odd size)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861280b",
   "metadata": {},
   "source": [
    "## Sample Distributions\n",
    "\n",
    "What we saw was the results of running one sample.  This sample was randomly chosen.  If we randomly choose another sample, this will have different statistics.\n",
    "\n",
    "So why don't we run a number of tests, then look at the distribution of the scores?\n",
    "\n",
    "After showing us the tedious way to split our dataset into test and training subsets, the book then went on to say that scikit-learn has a method for doing this split: _train_test_split_.  We will use this function for our test (but we have to import that function).  One thing to note is that in our old code, the percentage we used was the percentage to put in the training set.  For this new method, the percent is the amount to put in the testing set!  So we will change the _PRC_ value accordingly.\n",
    "\n",
    "The other thing that is a bit weird is that they called the result 'Mean expected error', but really, this is the 'Mean expected accuracy'.  Not sure why they did that!  So I changed the label!\n",
    "\n",
    "Finally, one other thing: they ran the test with _K = 1_ which we know is pretty poor.  So I switched to using _K = 11_, so the numbers here look better than what the book shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "PRC = 0.3\n",
    "num_tests = 10\n",
    "acc = np.zeros((num_tests,))\n",
    "for i in range(num_tests):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = PRC)\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors = 9)\n",
    "    knn.fit(X_train, Y_train)\n",
    "    yhat = knn.predict(X_test)\n",
    "    acc[i] = 100 * metrics.accuracy_score(yhat, Y_test)\n",
    "acc.shape = (1, num_tests)\n",
    "print(f'Mean expected accuracy: {np.mean(acc[0]):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d334f139",
   "metadata": {},
   "source": [
    "__Tiny Digression__\n",
    "\n",
    "In the above examples, we were using a 70/30 split for the training and testing set sizes.  In the next example in the book, they switched to a 90/10 split.  I was just curious, how does that affect the accuracy?  So I ran the following test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_percents = 10\n",
    "num_tests = 20\n",
    "results = []\n",
    "xvals = []\n",
    "for j in range(num_percents):\n",
    "    PRC = 0.5 * (j + 1) / num_percents\n",
    "    xvals.append(PRC)\n",
    "    acc = np.zeros((num_tests,))\n",
    "    for i in range(num_tests):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = PRC)\n",
    "        knn = neighbors.KNeighborsClassifier(n_neighbors = 9)\n",
    "        knn.fit(X_train, Y_train)\n",
    "        yhat = knn.predict(X_test)\n",
    "        acc[i] = 100 * metrics.accuracy_score(yhat, Y_test)\n",
    "    acc.shape = (1, num_tests)\n",
    "    results.append(np.mean(acc[0]))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "plt.plot(xvals, results);\n",
    "plt.ylabel('Mean Expected Accuracy')\n",
    "plt.xlabel('Test Size (as percent of whole set)')\n",
    "plt.xticks(xvals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d761e54",
   "metadata": {},
   "source": [
    "I actually ran this test several times, and even tried varying the number of tests.  The graph came out looking completely different each time.  On the other hand, all of these values are within one percent.  So it seems like the percent pulled aside didn't matter that much.  However, I think for the really small percentages (<10%), there was a lot more variability, probably because there just weren't enough datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b11ea",
   "metadata": {},
   "source": [
    "## Other Classifiers\n",
    "\n",
    "For the next investigation, the book added additional classifiers, specifically the _Support Vector Machines_ (SVM) and _Decision Trees_.  After importing these packages, these new classifiers can be dropped in (although tuning parameters may improve the results).  Each of the classifiers has the same _fit_ and _predict_ interface.\n",
    "\n",
    "One change: they used _K = 1_ and _K = 3_ for the KNN classifier.  I switched to use _K = 5_ and _K = 11_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55dda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "PRC = 0.1\n",
    "acc_r = np.zeros((10,4))\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = PRC)\n",
    "    nnA = neighbors.KNeighborsClassifier(n_neighbors = 5)\n",
    "    nnB = neighbors.KNeighborsClassifier(n_neighbors = 11)\n",
    "    svc = svm.SVC()\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    nnA.fit(X_train,y_train)\n",
    "    nnB.fit(X_train,y_train)\n",
    "    svc.fit(X_train,y_train)\n",
    "    dt.fit(X_train,y_train)\n",
    "    \n",
    "    yhat_nnA = nnA.predict(X_test)\n",
    "    yhat_nnB = nnB.predict(X_test)\n",
    "    yhat_svc = svc.predict(X_test)\n",
    "    yhat_dt = dt.predict(X_test)\n",
    "    \n",
    "    acc_r[i][0] = metrics.accuracy_score(yhat_nnA, y_test)\n",
    "    acc_r[i][1] = metrics.accuracy_score(yhat_nnB, y_test)\n",
    "    acc_r[i][2] = metrics.accuracy_score(yhat_svc, y_test)\n",
    "    acc_r[i][3] = metrics.accuracy_score(yhat_dt, y_test)\n",
    "\n",
    "\n",
    "plt.boxplot(acc_r);\n",
    "for i in range(4):\n",
    "    xderiv = (i+1)*np.ones(acc_r[:,i].shape)+(np.random.rand(10,)-0.5)*0.1\n",
    "    plt.plot(xderiv,acc_r[:,i],'ro',alpha=0.3)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.set_xticklabels(['5-NN','11-NN','SVM','Decision Tree'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7d821",
   "metadata": {},
   "source": [
    "For this particular data set and for this example, it looks like KNN(11) is our best bet, but SVN is very close."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5904615",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "The book next investigated the effect of number of examples on the test and training errors, and they used a Decision Tree as the classifier.\n",
    "\n",
    "They also used a new dataset, a Toy problem, built as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "MAXN = 700\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(6,5)\n",
    "\n",
    "plt.plot(1.25*np.random.randn(MAXN,1),1.25*np.random.randn(MAXN,1),'r.',alpha = 0.3)\n",
    "plt.plot(8+1.5*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2),'r.', alpha = 0.3)\n",
    "plt.plot(5+1.5*np.random.randn(MAXN,1),5+1.5*np.random.randn(MAXN,1),'b.',alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac769c2f",
   "metadata": {},
   "source": [
    "They generated the above plot to be representative of the data, but they generate different datasets below.  They will run 10 passes through the system, each randomly generated.  But all of the samples have the same general form.\n",
    "\n",
    "One parameter they can change is the _complexity_ of the Decision Tree.  The complexity is related to the depth of the tree.  One tuning parameter for the Decision Tree is the maximum depth.  By limiting this depth, the tree might have to stop segregating the values early, so there are fewer tests during evaluation, but there is some error due to less segregating of the data.\n",
    "\n",
    "We'll see the graph first, then we'll discuss how the graph was built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "C=5\n",
    "MAXN=1000\n",
    "\n",
    "yhat_test=np.zeros((10,299,2))\n",
    "yhat_train=np.zeros((10,299,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(10):\n",
    "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
    "    j=0\n",
    "    for N in range(10,3000,10):\n",
    "        Xr=X[:N,:]\n",
    "        yr=y[:N]\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(Xr,yr.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'pink')\n",
    "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1,p2],[\"Test C = 5\",\"Train C = 5\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10315f",
   "metadata": {},
   "source": [
    "They started by making a vector of training data and a vector of test data.  Each of these sets consisted of 1000 random points centered around 0,0 (belonging to the 'Yes' group), 1000 points centered around 5,5 (belonging to the 'No' group), and 1000 points centered around 8,5 (belonging to the 'Yes' group).  They then permuted these arrays.\n",
    "\n",
    "Next, they ran a series of evaluations.  For each, they selected _n_ of the training points, but all of the test points.  _n_ varied from 10 to 3000 in steps of 10.  So for the first several evaluations, they had very little training data!\n",
    "\n",
    "The lower curve of the graph shows the error rate for predicting the training data, and the upper curve is the test data.\n",
    "\n",
    "Note for really small sample sizes, the classifier did a great job on the training data.  Consider the problem!  There are three clusters of points.  All of the points centered around 0,0 are clearing in group 'Yes', so it is not problem identifying them.  But the other two clusters partially overlap.  In the heart of the overlap region, how can the classifier correctly separate essentially identical values.  It looks like there is a built-in error rate of about 10% due to this region of overlapping points.\n",
    "\n",
    "* As the number of samples increases, both curves tend towards the same value.\n",
    "\n",
    "* When we have few training data, the training prediction has a low error rate, but the test predition has a high error rate.\n",
    "\n",
    "Next they repeated the test with a less-complex tree, the depth is limited to 1 (the previous was 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5aa2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1\n",
    "MAXN=1000\n",
    "\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(10):\n",
    "    X = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y = np.concatenate([y,np.ones((MAXN,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(MAXN,2),5+1.5*np.random.randn(MAXN,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(MAXN,2)])\n",
    "    y_test = np.concatenate([np.ones((MAXN,1)),-np.ones((MAXN,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((MAXN,1))])\n",
    "    j=0\n",
    "    for N in range(10,3000,10):\n",
    "        Xr=X[:N,:]\n",
    "        yr=y[:N]\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(Xr,yr.ravel())\n",
    "        yhat_test[i,j,1] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,1] = 1. - metrics.accuracy_score(clf.predict(Xr), yr.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
    "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p3,p4],[\"Test C = 1\",\"Train C = 1\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81b6ea",
   "metadata": {},
   "source": [
    "With this simpler classifier, the system 'settled' much quicker, but at a much higher error rate, around 33%.  Clearly the trees did not have enough depth to adequately distinguish the samples.\n",
    "\n",
    "In this next graph, we show both on the same graph, to more clearly show the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1,=plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),color='pink')\n",
    "p2,=plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'c')\n",
    "p3,=plt.plot(np.mean(yhat_test[:,:,1].T,axis=1),'r')\n",
    "p4,=plt.plot(np.mean(yhat_train[:,:,1].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Number of samples x10')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1,p2,p3,p4],[\"Test C = 5\",\"Train C = 5\",\"Test C = 1\",\"Train C = 1\"])\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716be20",
   "metadata": {},
   "source": [
    "* With a low degree of complexity, the training and test errors converge to the bias sooner, with fewer data.\n",
    "\n",
    "* With a low degree of complexity, the error of convergence is larger than with increased complexity.\n",
    "\n",
    "The value that both curves converge toward is called the _bias_, and the difference between this value and the test error is called the _variance_.\n",
    "\n",
    "The previous tests were varying the number of samples.  In the next test, they use a fixed number of samples, but vary the complexity of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "\n",
    "MAXC=20\n",
    "N=1000\n",
    "NTEST=4000\n",
    "ITERS=3\n",
    "\n",
    "yhat_test=np.zeros((ITERS,MAXC,2))\n",
    "yhat_train=np.zeros((ITERS,MAXC,2))\n",
    "#Repeat ten times to get smooth curves\n",
    "for i in range(ITERS):\n",
    "    X = np.concatenate([1.25*np.random.randn(N,2),5+1.5*np.random.randn(N,2)]) \n",
    "    X = np.concatenate([X,[8,5]+1.5*np.random.randn(N,2)])\n",
    "    y = np.concatenate([np.ones((N,1)),-np.ones((N,1))])\n",
    "    y = np.concatenate([y,np.ones((N,1))])\n",
    "    perm = np.random.permutation(y.size)\n",
    "    X = X[perm,:]\n",
    "    y = y[perm]\n",
    "\n",
    "    X_test = np.concatenate([1.25*np.random.randn(NTEST,2),5+1.5*np.random.randn(NTEST,2)]) \n",
    "    X_test = np.concatenate([X_test,[8,5]+1.5*np.random.randn(NTEST,2)])\n",
    "    y_test = np.concatenate([np.ones((NTEST,1)),-np.ones((NTEST,1))])\n",
    "    y_test = np.concatenate([y_test,np.ones((NTEST,1))])\n",
    "\n",
    "    j=0\n",
    "    for C in range(1,MAXC+1):\n",
    "        #Evaluate the model\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(X,y.ravel())\n",
    "        yhat_test[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        yhat_train[i,j,0] = 1. - metrics.accuracy_score(clf.predict(X), y.ravel())\n",
    "        j=j+1\n",
    "\n",
    "p1, = plt.plot(np.mean(yhat_test[:,:,0].T,axis=1),'r')\n",
    "p2, = plt.plot(np.mean(yhat_train[:,:,0].T,axis=1),'b')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,5)\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Error rate')\n",
    "plt.legend([p1, p2], [\"Testing error\", \"Training error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dcec6",
   "metadata": {},
   "source": [
    "As we can see, as the complexity increases, the training error is reduced.  But above a certain level of complexity, the test error increases.  This effect is called _overfitting_.  Here are some techniques to overcome overfitting:\n",
    "\n",
    "* Run an analysis like we did here, then choose the parameter value (in this case depth of the trees) that minimizes the error.  This is a form of model selection.\n",
    "\n",
    "* Modify the model to penalize high-complexity. The book was a little confusing on this point!\n",
    "\n",
    "* Use 'ensemble' techniques, such as the Random Forest, where we don't rely on a single classifier (Decision Tree in this case) to match all aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8219d4a",
   "metadata": {},
   "source": [
    "## Training, Validation, and Test\n",
    "\n",
    "So far, we have been dividing our sample data into two sets, training and test.  With these two subsets, we can perform cross-validation to pick the best model to use for the data, or we can use cross-validation to tune a model (for example, selecting the best complexity for a Decision Tree).  But we can't do both, because these two optimizations would be correlated.\n",
    "\n",
    "We could, instead, divide our sample data into three sets, _training_, _validation_, and _test_.\n",
    "\n",
    "* The training data is used to select model from our list of possible models.\n",
    "\n",
    "* The validation data is used to select the parameters of the model for the best performance.  This is a form of learning!\n",
    "\n",
    "* The test data is used exclusively for assessing the performance at the end of the process and is never used in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaabad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
