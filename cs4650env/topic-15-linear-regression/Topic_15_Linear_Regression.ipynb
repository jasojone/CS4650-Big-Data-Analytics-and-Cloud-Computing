{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5672a6",
   "metadata": {},
   "source": [
    "# Topic 15: Linear Regression\n",
    "\n",
    "Linear regression is the most commonly used machine learning algorithm.\n",
    "\n",
    "The basic concept is quite simple, but there are many variations and extensions upon the ideas, and many implementations for performing linear regression.\n",
    "\n",
    "We will see a summary of these techniques in this topic.  For those who are interested in data science as a carreer, there is a lot more to learn!\n",
    "\n",
    "Here is an overview:\n",
    "\n",
    "* The basic idea\n",
    "\n",
    "* Definitions and Terms\n",
    "\n",
    "* Simple Linear Regression\n",
    "\n",
    "* Building Larger Test Cases\n",
    "\n",
    "* Polynomial Linear Regressions\n",
    "\n",
    "* Building Larger Test Cases\n",
    "\n",
    "* Using Sklearn\n",
    "\n",
    "* Multiple Linear Regressions (Multiple Inputs)\n",
    "\n",
    "* Sparse Data\n",
    "\n",
    "But first, we will set up the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a3503",
   "metadata": {},
   "source": [
    "In a previous lecture, I said I would explain the purpose of the '%maplotlib inline' line of text.  This is a magic command in Jupyter which links the charts and plots from matplotlib to the Jupyter Notebook, so that the charts appear in the cell outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e95eb",
   "metadata": {},
   "source": [
    "## The Basic Idea\n",
    "\n",
    "Suppose we have a sequence of data values, and we would like to determine if there is a simple mathematical relationship between the values.  _For example, the relationship between wind speed and measured wind-chill factor_.\n",
    "\n",
    "Or suppose the sequence represents the results achieved with application of a given input value. _The example several sources used was number of hours studied as input and the grade received as output_.\n",
    "\n",
    "Perhaps we would like to then estimate, for a different input value, what output value should be expected?\n",
    "\n",
    "In the following, we have a set of output values for various input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ec13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_x = range(0, 10)\n",
    "intro_y = [-70, -11, -13, 37, -13, 73, 80, 152, 147, 180]\n",
    "plt.scatter(intro_x, intro_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2caa80",
   "metadata": {},
   "source": [
    "One common approach is to find the _best line_ that \"fits\" our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c34080",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_z = np.polyfit(intro_x, intro_y, 1)\n",
    "intro_p = np.poly1d(intro_z)\n",
    "plt.scatter(intro_x, intro_y)\n",
    "plt.plot(intro_x, intro_p(intro_x), 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e61c47",
   "metadata": {},
   "source": [
    "Using the least-squares fit, the green line provides the best line approximation of the data.  Not all of the points fit exactly on the line (and in fact there is no straight line that exactly holds all of the points).  The distance between any point and the line is the _error_ for that point, and the error for the whole line is the sum of the squares of the errors for every point.  Any other single line that we would draw would have a larger error.\n",
    "\n",
    "sklearn has a built-in method which computes the _mean squared error_, so we can get a measure of how well the line fits the data.  Many times, we don't really want the mean squared error, but the square root of that value (so that the number uses the same units as the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.sqrt(mean_squared_error(intro_p(intro_x), intro_y)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e0071",
   "metadata": {},
   "source": [
    "Using this line, we could estimate the output for any input.  So, for example, if the input was 1.5, the output would be approximately -24 (I just eyeballed this from the graph!).\n",
    "\n",
    "We could also estimate the output if the input were 10, although this is beyond the range of our input data.  If we look too far beyond the range of our data, that output value may be suspect!  There may be other consideration that would limit the results.  For example, if we were estimating the grade received based on study time spent, we might, if we put in a large enough study time, estimate a grade of 220%, or for the crop example, if we put in a low enough amount of water expect a negative size of crops!  However, using an input value of 10 is close to our data range, so we would expect the prediction to be reasonable.\n",
    "\n",
    "There is one other point to cover about our plots:  I usually end the cells containing a plot with the command 'plt.show()'.  This is not really required, as you can see here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(intro_x, intro_y)\n",
    "plt.plot(intro_x, intro_p(intro_x), 'g-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fe1a6",
   "metadata": {},
   "source": [
    "As you can see, the plot still draws.  However, look at that line, '[<matplotlib.lines.Line2D at ...>]'.  What is that all about?  Usually Jupyter prints out the last expression or last result of the cell, then if there was a plot, it prints that.  So we get those ugly lines in our Notebook.  By ending a cell containing a plot with 'plt.show()', this removes that text.\n",
    "\n",
    "With this introduction, let's define some terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee51e5",
   "metadata": {},
   "source": [
    "## Definitions and Terms\n",
    "\n",
    "For the simplest linear regression, we are trying to fit the data to a line.\n",
    "\n",
    "The equation for a line is: $ y = m * x + b $.\n",
    "\n",
    "In that equation:\n",
    "\n",
    "* _x_ is an input value,\n",
    "* _y_ is the associated output value,\n",
    "* _m_ is the slope of the line, and\n",
    "* _b_ is the intercept, which is where the line crosses the y-axis (where x == 0).\n",
    "\n",
    "The goal of simple linear regression is to compute the _m_ and _b_ values given the training set of input values and output values.\n",
    "\n",
    "For our example above, _m_ = 26.64, and _b_ is -63.69.\n",
    "\n",
    "While most of the math world calls _m_ the _slope_, in the machine learning community, this is often called the _regression coefficient_.\n",
    "\n",
    "_x_ is called the _input variable_.  Alternate names are _independent variable_, _predictor variable_, or _feature_.\n",
    "\n",
    "_y_ is called the _output variable_.  Alternate names are _dependent variable_, _predicted variable_, or _target_.\n",
    "\n",
    "A _prediction_ is the value that the linear regression model computes for a given input value.  Sometimes we pass an array of input values, and receive an array of the corresponding output values.  In this case _prediction_ refers to the whole array of output values.\n",
    "\n",
    "An _outlier_ is an unexpected value, a value that is far outside the expected range.  In the following, we add an outlier to the graph.  Since _x_ is a _range_, we can't really add a new element, so I've converted these to a Pandas Series, then concatenated a value which is an outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pair of Series:\n",
    "outlier_x = pd.Series(intro_x)\n",
    "outlier_y = pd.Series(intro_y)\n",
    "# The outlier.  This must also be a pair of Series\n",
    "x_add = pd.Series([10])\n",
    "y_add = pd.Series([12])\n",
    "# Concatenate the Series, so now we have the outlier with all of the other values\n",
    "outlier_x = pd.concat([outlier_x, x_add], ignore_index=True)\n",
    "outlier_y = pd.concat([outlier_y, y_add], ignore_index=True)\n",
    "# Do the math:\n",
    "outlier_z = np.polyfit(outlier_x, outlier_y, 1)\n",
    "# Turn this into an evaluator\n",
    "outlier_p = np.poly1d(outlier_z)\n",
    "# Generate the plot\n",
    "plt.scatter(outlier_x, outlier_y)\n",
    "plt.plot(outlier_x, outlier_p(outlier_x), 'r-')\n",
    "plt.plot(intro_x, intro_p(intro_x), 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ffa54d",
   "metadata": {},
   "source": [
    "The green line was the original solution, the red line shows the solution with this outlier included.  You can see how the one value really skews the line!  Consequently, we can see that with linear regression, it is important that we clean the data, removing any outliers, or samples with missing values, from the dataset.\n",
    "\n",
    "We can also look at the mean squared error for the red line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e39e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.sqrt(mean_squared_error(outlier_p(outlier_x), outlier_y)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab9778",
   "metadata": {},
   "source": [
    "\n",
    "The general form of the equation for the regression is\n",
    "\n",
    "$$ y = \\Omega_0 + \\Omega_1 x^1 + \\Omega_2 x^2 ... \\Omega_n x^n $$\n",
    "\n",
    "The _degree_ of the polynomial is the largest exponent of the input variable.  For a line, the degree would be 1, but for a parabola (for example), the degree would be 2, and so on.  The $\\Omega$ values are the _coefficients_ of the equation, and any of these may be 0 (except for the coefficient for the largest exponent).\n",
    "\n",
    "One interesting thing to note: In mathematics, we would normally write the higher-order terms on the left and the lower-order terms on the right.  In data analytics, they reversed this convention!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124f3028",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "There are several tools in the Anaconda/Miniconda space that can perform linear regression.  Later we will be using _LinearRegression_ from scikit_learn, but for simple linear regressions, many people prefer the solution in Numpy, _polyfit_:\n",
    "\n",
    "* _polyfit_ is more elegant,\n",
    "* _polyfit_ is easier to learn, new users find _LinearRegression_ more confusing.\n",
    "* _polyfit_ seems to be more stable.  _LinearRegression_ changes much more frequently.  If you build a solution using _LinearRegression_, you may find that the code is updated, so your solution is out-of-date.\n",
    "\n",
    "In a little bit, we will see examples of _LinearRegression_, so you can decide for yourself whether the above points apply to you!\n",
    "\n",
    "Let's duplicate our earlier example, to explain the steps involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd67423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_x = range(0, 10)\n",
    "ex1_y = [-70, -11, -13, 37, -13, 73, 80, 152, 147, 180]\n",
    "ex1_z = np.polyfit(ex1_x, ex1_y, 1)\n",
    "ex1_p = np.poly1d(ex1_z)\n",
    "plt.scatter(ex1_x, ex1_y)\n",
    "plt.plot(ex1_x, ex1_p(ex1_x), 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50819694",
   "metadata": {},
   "source": [
    "* The first two lines are our sample data.  Normally, we would fetch (larger) datasets from .csv files, or other sources of data.  For this small example, we simply enter the values as a range and a Python array.  The *ex1_x* values are integers running from 0 through 9 (it stops before 10).  The *ex1_y* values are explicitly listed.\n",
    "\n",
    "* The third line is the actual machine learning!  The _polyfit_ routine is passed an array of input values, an array of output values, and the degree of the desired polynomial.  In this case, the degree is 1, we are looking for a line.  The return value is an array, giving the coefficients of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6fa0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d24fa",
   "metadata": {},
   "source": [
    "* The fourth line turns the array into an object that computes that polynomial: given an input value, it computes the corresponding output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939f868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex1_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16834556",
   "metadata": {},
   "source": [
    "* The remaining lines generate the plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a153d8b",
   "metadata": {},
   "source": [
    "## Building Larger Test Cases\n",
    "\n",
    "We entered a small test case by hand, and that was useful as an introduction.  But in many cases we might like to generate a larger test case, using random values.  sklearn has a tool, _datasets.make_regression_, that can create such test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_n_samples = 50\n",
    "ex2_x, ex2_y, ex2_coef = datasets.make_regression(\n",
    "    n_samples = ex2_n_samples,\n",
    "    n_features = 1,\n",
    "    n_informative = 1,\n",
    "    noise = 5,\n",
    "    coef = True,\n",
    "    random_state = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610e77b",
   "metadata": {},
   "source": [
    "* *n_samples* indicates the number of samples in the dataset to be generated.\n",
    "* *n_features* indicates the number of inputs.  We are currently using 1, but later we will use more.\n",
    "* *n_informative* indicates the number of those inputs that are 'important', that really affect the output values.  Any non-informative inputs do not influence the output, but simply confuse the regressor.\n",
    "* Not shown here, *n_targets* indicates the number of output values.  By default, the value is 1.\n",
    "* _noise_ indicates how much randomness should be added, how scattered are the output values.\n",
    "* _coef_ indicates whether the coefficients of the generator should be returned.  These indicate the formula used to create the points, and ideally these are the values that the regression should compute.\n",
    "* *random_state* is the seed value for the random number generator.  By specifying this value, then subsequent runs will generate the same sequence of random values.  Hence, what you see in your Notebook should match the values in my Notebook.\n",
    "\n",
    "Let's see what got generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ex2_x, ex2_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeefbcf",
   "metadata": {},
   "source": [
    "This looks like a reasonable collection of data points.  We could ask for more randomness, or fewer or more points, and so on.\n",
    "\n",
    "We can look to see what the return value looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77138471",
   "metadata": {},
   "source": [
    "We see that *ex2_x*, the inputs to the regression, is a 2-dimensional array.  The outer dimension or axis has one entry for every _row_ of our data, and the inner dimension, which is just 1 long, gives the number of input values.\n",
    "\n",
    "We will see that the *ex2_y* value, the output, is just a 1-dimensional array.  If we asked for multiple targets, the output would also be a 2-dimensional array,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbeb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4a4b8",
   "metadata": {},
   "source": [
    "*ex2_x* is a 2-dimensional array, but for some of the uses coming up, we want this as a 1-dimensional array.  There are a number of ways we can reconfigure an array.  Here we use the Numpy _reshape_ method.  We pass in an array, then a description of the shape we want: an array of integers giving the number of elements in each dimension.  We only want one dimension, and the number of elements we want 'in that dimension' is the size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3afef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_x = np.reshape(ex2_x, [ex2_x.size])\n",
    "ex2_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c06567",
   "metadata": {},
   "source": [
    "Now that we have the data in the format we want, we can perform the regression, generate the formula, and build the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_z = np.polyfit(ex2_x, ex2_y, 1)\n",
    "ex2_p = np.poly1d(ex2_z)\n",
    "plt.scatter(ex2_x, ex2_y)\n",
    "plt.plot(ex2_x, ex2_p(ex2_x), 'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d52af6",
   "metadata": {},
   "source": [
    "We can see the equation for this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00bf8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex2_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f08a58",
   "metadata": {},
   "source": [
    "We can also evaluate that line at any input value, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2defe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_p(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847480b4",
   "metadata": {},
   "source": [
    "## Polynomial Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356dd0a6",
   "metadata": {},
   "source": [
    "Sometimes we have data that does not fit along a line, but rather along a curve.  Before we start, let's build a convenience function that will perform an analysis on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_regress(x_data, y_data, degree) :\n",
    "    z = np.polyfit(x_data, y_data, degree)\n",
    "    p = np.poly1d(z)\n",
    "    yhat_data = p(x_data)\n",
    "    plt.scatter(x_data, y_data)\n",
    "    plt.plot(x_data, yhat_data, 'b-')\n",
    "    plt.show()\n",
    "    print(f'Mean-square error: {np.sqrt(mean_squared_error(yhat_data, y_data)):.2f}')\n",
    "    print('Formula:\\n', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e5bca",
   "metadata": {},
   "source": [
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d706d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = range(1, 26)\n",
    "y_a = [1.2, 0.8, 1, 0.9, 0.9, 1.3, 1, 1.7, 1.2, 1.8, 2, 1.8, 2, 3.2, 3.3, 4.2, 5.1, 5.2, 6, 7, 9, 10.1, 12, 14, 16]\n",
    "poly_regress(x_a, y_a, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629a34ce",
   "metadata": {},
   "source": [
    "As we can see, the straight line does a really poor job of fitting the set of data points.  Visually we see the mismatch, and the mean-square error is pretty bad.  _Note that this number is a lot smaller than the numbers we saw above, but that is not a surprise, since the numbers in this curve are a lot smaller._\n",
    "\n",
    "With that evaluation, we were using a polynomial of degree 1.  Recall that the degree of a polynomial is the exponent of the largest power of _x_.  Consequently, our polynomial looked like this:\n",
    "\n",
    "$$ y = \\Omega_0 + \\Omega_1 x^1 $$\n",
    "\n",
    "Now let's try a polynomial of degree 2, which is a parabola.  The polynomial will look like this:\n",
    "\n",
    "$$ y = \\Omega_0 + \\Omega_1 x^1 + \\Omega_2 x^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b336f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_a, y_a, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d8ddd",
   "metadata": {},
   "source": [
    "That is looking a lot better!  The line (curve) follows the points a lot better, and the error is a lot smaller.\n",
    "\n",
    "Feeling quite happy about these results, we next try using a third-degree polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64eda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_a, y_a, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deed1f5",
   "metadata": {},
   "source": [
    "That is looking even better!  The error has gone down even more, and the line is a much closer match to the points.\n",
    "\n",
    "So let's try a fourth-degree polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_a, y_a, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258100b",
   "metadata": {},
   "source": [
    "This is only a slight improvement over the third-degree polynomial, so perhaps we have gone far enough.  Also note that the leading coefficient is really small, hinting that this term is not strongly influencing the result.\n",
    "\n",
    "These examples were following a simple curve.  Let's try a curve with an inflection point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4dc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b = range(1, 20)\n",
    "y_b = [-6.5, -5.7, -3.5, -1.5, -0.8, .5, 1.2, 1.1, .8, 1.1, 1, 1.2, 2, 2, 3.2, 4.8, 6.4, 6.8, 8.6]\n",
    "poly_regress(x_b, y_b, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d27bc",
   "metadata": {},
   "source": [
    "As before, not too good.  Degree 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c04b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_b, y_b, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a06ba9",
   "metadata": {},
   "source": [
    "What!  The result is the same!  What happened?\n",
    "\n",
    "OK, the result is not exactly the same, we can see in the formula that there is a small influence by the leading term.  But the error didn't improve.\n",
    "\n",
    "The problem is, there is no parabola that fits this set of points (OK, technically, yes there is, but not really).\n",
    "\n",
    "Let's move on to the third degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5284b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_b, y_b, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ba330",
   "metadata": {},
   "source": [
    "Now we're doing pretty good.  The curve really seems to fit the points, and the error has gone way down.\n",
    "\n",
    "4th degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628d97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_b, y_b, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd5f54",
   "metadata": {},
   "source": [
    "Again, not much of an improvement.  We should stick with a third-degree polynomial.\n",
    "\n",
    "Let's try another curve, a little more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667788ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c = range(1, 19)\n",
    "y_c = [8.8, 6.8, 4.7, 2.8, 2.1, 3, 3.7, 3.6, 2.5, 1.5, 0.7, 0.6, 1.0, 1.8, 2.9, 4.2, 6, 7.5]\n",
    "poly_regress(x_c, y_c, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311a39c",
   "metadata": {},
   "source": [
    "Nope!  2nd-degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49610fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a3797",
   "metadata": {},
   "source": [
    "Better, but needs some work!\n",
    "\n",
    "3rd degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba819eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f2965",
   "metadata": {},
   "source": [
    "This looks a lot like the last solution.  There is not a good fit for a 3rd degree polynomial.  4th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ffcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_c, y_c, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e22de0",
   "metadata": {},
   "source": [
    "OK, we finally got a nice fit to the curve.  The problem is, if we use a polynomial of too high a degree, we are _overfitting_ the data, we are matching the variance of the data, rather than the general trend of the data.  Consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_d = range(1, 19)\n",
    "y_d = [12, 8, 9, 7, 6, 3, 2, 4, 3, 1, 0, 1.5, 2.7, .8, 2.2, 1, 1.2, 2]\n",
    "poly_regress(x_d, y_d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a760921",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_d, y_d, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd914f",
   "metadata": {},
   "source": [
    "We probably would prefer the first solution, which generally matched the shape of the curve, rather than the second solution, which directly connected each of the points.  That second solution also captured and reproduced the 'noise'.\n",
    "\n",
    "If we think back to our first cases, we noticed that for a while we were getting big reductions in the error measurement, but that after a point, the error measure didn't change by much.  This is a typical pattern.\n",
    "\n",
    "We can pursue a new direction here: Why don't we graph the error as a function of the degree of the polynomial.\n",
    "However, before we do, let's turn off the warnings.  Why?  In the following, we are going to ask polyfit to run over a wide range of degrees, and when we give really large degree, it prints a warning message about polyfit being poorly conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c263efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990b363",
   "metadata": {},
   "source": [
    "So now we implement, then run, a routine to test out various degrees of polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_degree(x_data, y_data) :\n",
    "    num_degrees = 30\n",
    "    scores = []\n",
    "    rng = range(1, num_degrees)\n",
    "    for degree in rng:\n",
    "        z = np.polyfit(x_data, y_data, degree)\n",
    "        p = np.poly1d(z)\n",
    "        yhat_data = p(x_data)\n",
    "        scores.append(np.sqrt(mean_squared_error(yhat_data, y_data)))\n",
    "    plt.plot(rng, scores, 'b-')\n",
    "\n",
    "\n",
    "map_degree(x_a, y_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3594a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_degree(x_b, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_degree(x_c, y_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_degree(x_d, y_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37c3b06",
   "metadata": {},
   "source": [
    "These graphs show that there is a \"knee\" (or is it \"elbow\").  Before the elbow the error rate is rapidly dropping.  After that, it tapers off a bit.  Yes, the formula gets more accurate past that point, but that is because of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76c761f",
   "metadata": {},
   "source": [
    "## Building Larger Test Cases\n",
    "\n",
    "In the previous section, we built toy examples, we hand-generated small lists of samples.  With a very limited number of samples, we couldn't do our normal train/test split of the data, so we could see how well the prediction worked on unseen examples.\n",
    "\n",
    "So let's write a function that will generate polynomial test cases for us.  This function should take the following values:\n",
    "* Coefficients for $ x^3, x^2, x^1 $ and $ x^0 $.\n",
    "* A factor for the randomness of the values\n",
    "* The number of points to generate\n",
    "* The low and high limits for the _x_ coordinates\n",
    "This function will return a DataFrame with two columns, the _x_ values and the corresponding _y_ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eeeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_regression_test(num_samples, x3coef, x2coef, x1coef, x0coef, random_factor, low_x, high_x) :\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(num_samples) :\n",
    "        x = np.random.random() * (high_x - low_x) + low_x\n",
    "        y0 = x * (x1coef + x * (x2coef + x * x3coef)) + x0coef\n",
    "        y = y0 + np.random.randn() * random_factor\n",
    "        x_vals.append(x)\n",
    "        y_vals.append(y)\n",
    "    return pd.DataFrame({'x': x_vals, 'y': y_vals}).sort_values(by = 'x', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c7b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_poly_regression_test(100, 0, 2, 3, 1, 10, 0, 20)\n",
    "poly_regress(df.x, df.y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_poly_regression_test(50, 1, -2, 0, 2.5, 10, -5, 5)\n",
    "poly_regress(df.x, df.y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafd712",
   "metadata": {},
   "source": [
    "Here we can see tests with a lot more data points, and we can see that the resulting formulas are fairly close to the actual generating formulas (at least the more significant terms, with the larger coefficients, are really close)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f271c",
   "metadata": {},
   "source": [
    "## Using Sklearn\n",
    "\n",
    "Numpy's _polyfit_ works nicely for the cases we used above, but perhaps we should learn how to use the sklearn _LinearRegression_ method as well.\n",
    "\n",
    "Here is one difference between the two:\n",
    "\n",
    "* _polyfit_ is designed to fit polynomials of a specified degree.  So given just the _x_ inputs, _polyfit_ will also compute the $ x^2 $ values, $ x^3 $ values, and so on, as needed based on the degree.\n",
    "\n",
    "* _LinearRegression_ is to handle multiple _features_ (input values), but only the single power of these features.\n",
    "\n",
    "To use _LinearRegression_ to fit a polynomial curve, we've got a problem.\n",
    "\n",
    "Luckily, there is a solution.  All we need to do, to fit a third-degree polynomial, is create two extra input columns in our DataFrame.  We started with the one column, containing the _x_ values.  We now need a second column filled with the square of the _x_ values, and a third column filled with the cube of the _x_ values.  _LinearRegression_ can then pick scaling factors for these three inputs, resulting in a polynomial fit!\n",
    "\n",
    "To make these extra columns, sklearn has a method called _PolynomialFeatures_.  Using this method is a two step process:\n",
    "\n",
    "1. Call _PolynomialFeatures_, passing in the desired degree of the polynomial.  This will return a generator.\n",
    "\n",
    "2. Call the generator, passing in the array of _x_ values.  This will return a 2-dimensional array, where the first column is the _x_ values, the second column is the $ x^2 $ values, and so on.\n",
    "\n",
    "Let's use this on our *x_b* data from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1692995",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b30bea",
   "metadata": {},
   "source": [
    "This is a _range_ object, but we want an array.  Numpy has a method for creating an array from a range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d45d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbb250",
   "metadata": {},
   "source": [
    "Getting back to PolynomialFeatures, let's create a generator that has a degree of 3 (for the *x_b* example, we want a polynomial of degree 3).  This function also takes a second parameter, *include_bias*, which we should always set to False (for linear regression, we don't want a 'constant term', the $x^0$ value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a89044",
   "metadata": {},
   "source": [
    "We can now run that generator to build our 'input array' for the linear regression, having columns for _x_, $x^2$, and $x^3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b_arr = poly.fit_transform(np.array(x_b).reshape(-1,1))\n",
    "x_b_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c5a1e",
   "metadata": {},
   "source": [
    "Now that we have these input columns, we can perform the LinearRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b83ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_reg_model = linear_model.LinearRegression()\n",
    "poly_reg_model.fit(x_b_arr, y_b)\n",
    "y_b_predicted = poly_reg_model.predict(x_b_arr)\n",
    "plt.scatter(x_b, y_b)\n",
    "plt.plot(x_b, y_b_predicted, 'b-')\n",
    "plt.show()\n",
    "print(f'Mean-square error: {np.sqrt(mean_squared_error(y_b_predicted, y_b)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11c748",
   "metadata": {},
   "source": [
    "Recalling the _polyfit_ solution, we see the similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regress(x_b, y_b, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aedb5a",
   "metadata": {},
   "source": [
    "Using sklearn was a bit more work, we needed to build the _PolynomialFeatures_, so there was a little more to keep track of.  But we will next see where the sklearn approach is quite useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c124dc",
   "metadata": {},
   "source": [
    "## Multiple Linear Regressions (Multiple Inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244a22c3",
   "metadata": {},
   "source": [
    "We will now build a dataset that has two inputs (features), but one of these does not strongly influence the output.  In other words, this input is some extraneous data that might simply confuse the regression.  This is a very common practice, as in real life, when we approach a dataset, we do not necessarily know which of the inputs really matter (at least for the particular output we are examining).\n",
    "\n",
    "For the following, we build a regression dataset.  This will have two features, but only one is informative (influences the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_n_samples = 50\n",
    "mlr_x, mlr_y, mlr_coef = datasets.make_regression(\n",
    "    n_samples = mlr_n_samples,\n",
    "    n_features = 2,\n",
    "    n_informative = 1,\n",
    "    noise = 2,\n",
    "    coef = True,\n",
    "    random_state = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33b477",
   "metadata": {},
   "source": [
    "The *mlr_x* result is a 2-dimensional array, containing the two input columns, and *mlr_y* is the output column.  For convenience we convert the input data into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_df = pd.DataFrame(mlr_x, columns = ['In1', 'In2'])\n",
    "mlr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9a1c9",
   "metadata": {},
   "source": [
    "Let's perform a Linear Regression using both of the two input values.  After we have finished the regression, we will plot the results using the 'In1' data (the informational data) for the x-coordinate, and then we will also plot the results using the 'In2' data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1347897",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = linear_model.LinearRegression()\n",
    "model1.fit(mlr_df, mlr_y)\n",
    "yhat = model1.predict(mlr_df)\n",
    "plt.scatter(mlr_df.In1, mlr_y)\n",
    "plt.plot(mlr_df.In1, yhat, 'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b086cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mlr_df.In2, mlr_y)\n",
    "plt.plot(mlr_df.In2, yhat, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc39e48",
   "metadata": {},
   "source": [
    "Well, this was surprising.  First of all, the 'In2' data is all over the map, it does not correlate with the 'In1' data nor with the 'Output' data.  Also, it was not sorted, so the lines go all over the place!\n",
    "\n",
    "We can do a quick sanity check: We can perform a linear regression using only In1, ignoring In2.  And then we can perform another linear regression using only In2, ignoring In1.  First, some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_regress(x_data, y_data, degree) :\n",
    "    model = linear_model.LinearRegression()\n",
    "    poly = PolynomialFeatures(degree = degree, include_bias = False)\n",
    "    x_data_arr = poly.fit_transform(np.array(x_data).reshape(-1,1))\n",
    "    model.fit(x_data_arr, y_data)\n",
    "    yhat = model.predict(x_data_arr)\n",
    "    plt.scatter(x_data, y_data)\n",
    "    plt.plot(x_data, yhat, 'b-')\n",
    "    plt.show()\n",
    "    print(f'Mean-square error: {np.sqrt(mean_squared_error(yhat, y_data)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823449e6",
   "metadata": {},
   "source": [
    "We now run a regression using only In1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_regress(mlr_df.In1, mlr_y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e186818e",
   "metadata": {},
   "source": [
    "That was what we expected, the results look good.  Now perform the regression using only In2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_regress(mlr_df.In2, mlr_y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4c1d3",
   "metadata": {},
   "source": [
    "This is also what we would expect: there is no really great correlation between In2 and the output value.\n",
    "\n",
    "So we have seen how the linear regression can have some extraneous input: the linear regression basically ignores this data.  Actually, the formula does consider this data, but the coefficient is really low.\n",
    "\n",
    "Here is one other thing to consider: What if the output really _did_ depend upon both of the input values?  In a more general case, there may be many input values, some of which should be ignored, and some should be used.  But in the simpler case, let's build a problem where the output depends upon two input values (and not upon a third):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9986c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr2_n_samples = 100\n",
    "mlr2_x, mlr2_y, mlr2_coef = datasets.make_regression(\n",
    "    n_samples = mlr2_n_samples,\n",
    "    n_features = 3,\n",
    "    n_informative = 2,\n",
    "    noise = 2,\n",
    "    coef = True,\n",
    "    random_state = 0,\n",
    ")\n",
    "mlr2_df = pd.DataFrame(mlr2_x, columns = ['In1', 'In2', 'In3'])\n",
    "model2 = linear_model.LinearRegression()\n",
    "model2.fit(mlr2_df, mlr2_y)\n",
    "yhat2 = model2.predict(mlr2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574653e0",
   "metadata": {},
   "source": [
    "OK, not so exciting (but theoretically all of the work has been done).\n",
    "\n",
    "Let's view the scatterplots of each input vs the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mlr2_df.In1, mlr2_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mlr2_df.In2, mlr2_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mlr2_df.In3, mlr2_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08f882",
   "metadata": {},
   "source": [
    "So each of the plots is really spread out.  In1 is a little reasonable.  But the error rate will probably be quite high.  Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean-square error: {np.sqrt(mean_squared_error(yhat2, mlr2_y)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5fbd1",
   "metadata": {},
   "source": [
    "Wow, that is really a small error!  Let's try doing independent regressions using each of the input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_regress(mlr2_df.In1, mlr2_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609cb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_regress(mlr2_df.In2, mlr2_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e33eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_regress(mlr2_df.In3, mlr2_y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a21018",
   "metadata": {},
   "source": [
    "The results are much like we would expect just looking at the scatterplots.  The output is not correlated with any _single_ input value, but _is_ highly correlated with the combination of In1 and In2, and is independent of In3.  We can print the coefficients that were used by the generator to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlr2_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b2ae0",
   "metadata": {},
   "source": [
    "## Sparse Data\n",
    "\n",
    "Often in real-world data, there are uninformative variables in the data (features which don't markedly affect the output).  With these data in the dataset, the regression will try to use these values to help fit the samples.  Usually the simpler model is preferred.\n",
    "\n",
    "For this section, we will use a database built-in to sklearn: The Boston housing market.  This dataset containes 13 features, and the question to be answered is: Can we predict the price of a new house given the values for these features?  The dataset also includes the median house price, so this column is the answer we will be attempting to predict.\n",
    "\n",
    "As a preliminary step, we will load the database.  We can then find the keys for this object (the keys are the names of the attributes for this object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()\n",
    "print('Keys:', boston.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eeee88",
   "metadata": {},
   "source": [
    "Based on these names, this is what we might expect:\n",
    "* _data_ is the list of inputs, or features.\n",
    "* _target_ is the target value, in this case the median house price.\n",
    "* *feature_names* is the column names for the input data.\n",
    "* _DESCR_ is a description of the dataset\n",
    "* The other two keys might not be as useful.\n",
    "\n",
    "Let's print the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f05b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dac25",
   "metadata": {},
   "source": [
    "This description gave us a lot of information about the dataset.  Let's build a DataFrame holding the data, and let's add another column to the table, named 'price', which is the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "df_boston['price'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029b6db",
   "metadata": {},
   "source": [
    "For our first visualization, let's plot a bar graph of the home prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b136bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_boston.price) \n",
    "plt.xlabel('price (\\$1000s)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69109bfa",
   "metadata": {},
   "source": [
    "We have 13 features in the dataset.  Some of these will probably be more significantly correlated to the price, while others may be insignificant.  We can build quick linear regressions between some columns and the output, although as we saw above, _combinations of features_ may correlate more strongly that these features in isolation.\n",
    "\n",
    "Seaborn has many interesting data visualization methods.  Check out the Seaboard documentation, they have a nice gallery showing representative diagrams.\n",
    "\n",
    "The main visualization we will use now is the linear regression, using _lmplot_.  We pass in the dataset, indicate which columns to use on the two axes, and give the size of the plot.\n",
    "\n",
    "For the first chart, let's compare _price_ with _LSTAT_ (the indication of lower income status):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d72803",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_boston, x=\"price\", y=\"LSTAT\", height = 5.2, aspect = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c8bcf",
   "metadata": {},
   "source": [
    "We can see that a linear equation does not fit the data too well, but that a curve would be better.  A quadratic might be a better fit.  While most places we would set the _degree_ of the polynomial, in Seaborn they used the name _order_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"price\", y=\"LSTAT\", data=df_boston, order=2, height = 5.2, aspect = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e45cf3",
   "metadata": {},
   "source": [
    "Another promising possibility is comparing the _price_ to _RM_, which is the number of rooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa54f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"price\", y=\"RM\", data=df_boston, height = 5.2, aspect = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf24daa5",
   "metadata": {},
   "source": [
    "For that pair, linear seems like a fairly good match.\n",
    "\n",
    "Next let's check the _CRIM_ column, an indication of the crime rate in the area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"price\", y=\"CRIM\", data=df_boston, height = 5.2, aspect = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"price\", y=\"CRIM\", data=df_boston, order = 2, height = 5.2, aspect = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d55a56",
   "metadata": {},
   "source": [
    "We've performed both a linear and quadratic regression.  Not sure which is best...\n",
    "\n",
    "## Heat Map\n",
    "\n",
    "We did some preliminary investigation by performing regressions between some interesting input columns and our output value.  These might give us some idea of which inputs to include and which to exclude when performing a multi-variable regression.\n",
    "\n",
    "Another approach to determine which inputs to use is to perform a correlation between the columns.  A correlation matrix checks all pairs of columns in the dataset, and for each pair computes the correlation.  If there is a strong correlation between one of the inputs and the output, then that is an input of interest.\n",
    "\n",
    "A DataFrame can compute the correlation matrix, and Seaborn can plot a Heat Map based on that matrix.  In the following, we select some of the columns, then produce the Heat Map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4c24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [0, 2, 4, 5, 6, 12]\n",
    "df2 = pd.DataFrame(boston.data[:,indexes], columns = boston.feature_names[indexes])\n",
    "df2['price'] = boston.target\n",
    "corrmat = df2.corr()\n",
    "sns.heatmap(corrmat, vmax = .8, square = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf7ea0",
   "metadata": {},
   "source": [
    "We are interested in the columns that are strongly correlated to the _price_.  Of course, each column is strongly correlated to itself!  But we can see that _price_ is strongly correlated to _RM_, the number of rooms.  _price_ is also strongly correlated to _LSTAT_, but this is a _negative correlation_: As _LSTAT_ goes up, _price_ tends to go down.  So in the Heat Map, we are looking for columns that are either really light colors or really dark colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b88f8",
   "metadata": {},
   "source": [
    "## Scatter Plot\n",
    "\n",
    "Another interesting visualization tool is a _scatter plot_.  For this, we first pick a small set of interesting columns.  We then build a new DataFrame containing just these columns (and our output _price_).  We then call the Pandas plotting *scatter_matrix* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5656bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [5, 6, 12]\n",
    "df2 = pd.DataFrame(boston.data[:,indexes], columns = boston.feature_names[indexes])\n",
    "df2['price'] = boston.target\n",
    "pd.plotting.scatter_matrix(df2, figsize=(12.0, 12.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d25322",
   "metadata": {},
   "source": [
    "On the diagonal of this diagram, we would be comparing each column with itself.  Instead, the program prints a bar chart of that column.  But in all of the non-diagonal cells, the diagram shows a scatter plot of the two columns (the row's column and the column's column!).\n",
    "\n",
    "In the cells along the right edge, we get an overview of how each of the columns relates to the price.\n",
    "\n",
    "However, in the other cells, we might find there is an interesting relationship between the two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a6dab",
   "metadata": {},
   "source": [
    "## LASSO\n",
    "\n",
    "Up to now, we have only been looking at the _training data_.  Because of this, we have no idea how the _predictive_ calculations of the model perform.  In the past, we split the data into _training_ and _testing_ sets.  We trained with the training set, but predicted with the testing set.\n",
    "\n",
    "We now do these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d26687",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = boston.data.shape[0]//2\n",
    "x_train = boston.data[:train_size]\n",
    "x_test = boston.data[train_size:]\n",
    "y_train = boston.target[:train_size]\n",
    "y_test = boston.target[train_size:]\n",
    "print('Training and testing set sizes', x_train.shape, x_test.shape)\n",
    "\n",
    "regr_boston = linear_model.LinearRegression()\n",
    "regr_boston.fit(x_train, y_train) \n",
    "print('Coeff and intercept:', regr_boston.coef_, regr_boston.intercept_)\n",
    "\n",
    "# Best possible score is 1.0, lower values are worse.\n",
    "print('Training Score:', regr_boston.score(x_train, y_train) )\n",
    "print('Testing Score:', regr_boston.score(x_test, y_test) )\n",
    "print('Training MSE: ', np.mean((regr_boston.predict(x_train) - y_train)**2))\n",
    "print('Testing MSE: ', np.mean((regr_boston.predict(x_test) - y_test)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a4b8a2",
   "metadata": {},
   "source": [
    "Well, it looks like our model is doing OK with the training data, but doing rather poorly with the testing data.  This is related to overfitting: our model is fairly well fit to the trained data.\n",
    "\n",
    "Recall when we looked at the heat map, and even when we did some spot checks, some of the features were correlated to the output to some degree, but other features were not.  For example, _RM_ and _LSTAT_ were informative, but _AGE_ was not.\n",
    "\n",
    "If we look at the array of coefficients, the model considered _all_ of the input features.  For every column, there is a non-zero coefficient.  Yes, some are small, but they all have an influence.\n",
    "\n",
    "We can build a _sparse model_: we only consider some of the features, and the remaining features are ignored.  To do this we can create a _LASSO_ regressor (least absolute shrinkage and selection operator), which forces some of the coefficients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_lasso = linear_model.Lasso(alpha=.3)\n",
    "regr_lasso.fit(x_train, y_train) \n",
    "print('Coeff and intercept:', regr_lasso.coef_,  regr_lasso.intercept_)\n",
    "\n",
    "print('Training Score:', regr_lasso.score(x_train, y_train))\n",
    "print('Testing Score:', regr_lasso.score(x_test, y_test))\n",
    "\n",
    "print('Training MSE: ', np.mean((regr_lasso.predict(x_train) - y_train)**2))\n",
    "print('Testing MSE: ', np.mean((regr_lasso.predict(x_test) - y_test)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808254c2",
   "metadata": {},
   "source": [
    "Because of the LASSO, the coefficient array has four entries that are zero: these features are ignored in the regression.  The factors dropped are _CRIM_ (the crime rate), _INDUS_ (percentage of industrial buildings), _CHAS_ (being located on the Charles River, and _NOX_ (nitrous oxide).  The testing score went from -2.24 to 0.5, and the MSE went from 303 to 47.  This shows that the four factors were not important for the prediction, and in fact they confused the regressor.\n",
    "\n",
    "We can get a report ranking the features in order of importance, from least to greatest, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6692af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argsort(np.abs(regr_lasso.coef_))\n",
    "print('Order variable (from less to more important):', boston.feature_names[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a87317",
   "metadata": {},
   "source": [
    "## Select K Best\n",
    "\n",
    "Another approach to finding the most important features is the _SelectKBest_ function from sklearn.\n",
    "\n",
    "This function takes an input parameter, _k_, which indicates the number of features we wish to include.  The function's output is an array listing all of the features, each with a value of True (keep this one) or False (ignore this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection as fs \n",
    "selector = fs.SelectKBest(score_func = fs.f_regression, k = 5)\n",
    "selector.fit_transform(x_train, y_train)\n",
    "z = list(zip(selector.get_support(), boston.feature_names))\n",
    "print('Selected features:', z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2f024",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've covered a lot of stuff.\n",
    "\n",
    "Recall that linear regression is the most-used machine learning algorithm.\n",
    "\n",
    "But there are several additional machine learning algorithms that we will be studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4fc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
